---
title: "PASSE analyses"
author: "Nick Beeton"
date: "13/08/2019"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(ape)
library(diversitree)
library(microbenchmark)
library(viridisLite)
library(fields)
library(rgl)
setwd('C:/Data/passe/')
source('functions.r')
dyn.load('passe.dll')
run = 1
pairs.cex = 0.7
pairs.lwd = 0.2
```

## Figure 1

In order to find sensible settings for $x$ and $y$, and to test PASSÉ more generally, we conducted a simulation study. We simulated 100 binary state phylogenetic trees with 1600 species, speciation rates $\lambda_0=0.2$ and $\lambda_1=1.8$, extinction rates $\mu_0=0.01$ and $\mu_1=0.8$, and transition rates $q_{01}=0.1$ and $q_{10}=0.1$. For each of these trees, the log-likelihood of the tree given the known parameters was calculated using BiSSE using the diversitree package in R, and using $x,y$-PASSÉ for all combinations of $x$ and $y$ with numbers in $(1,2,…9,10,20,30,…,100)$. The average difference in log-likelihood between BiSSE and each $x,y$-PASSÉ was taken as a measure of accuracy. The time taken for each of these process was also calculated, and the average proportion between the two or “speedup” then calculated. The set of Pareto-optimal values for PASSÉ were then found using the two optimality criteria of accuracy and speedup – that is, only those $x,y$-PASSÉ combinations were kept which are not dominated by another $x,y$-PASSÉ combination that is both faster and more accurate.

Figure 1 shows the accuracy (by log-likelihood difference) and speedup (relative to BiSSE) of different choices of $x$,$y$-PASSÉ on trees with 1600 tips. While 13 of the 19 $x=y$ cases tested were Pareto-optimal, 53 of the 171 $x>y$ cases were as opposed to only seven of the 171 $x<y$ cases. This confirms our assertion that devoting more computation time to terminating branches as opposed to non-terminating branches is a valid approach. However, the good performance for $x=y$ cases suggests that this effect is minor and further improvements such as adaptive time-steps are probably unnecessary. We chose $20,10-$PASSÉ to be our higher-accuracy version for further analyses, as it combines higher accuracy with a substantial approximate 10x speedup when compared to BiSSE.

```{r figure1}
splits = c(1L:10L, 10L*(2L:10L)) # number of splits to try
N.splits = length(splits)
N.reps = 100L
N.species = 1600L
pars = c(0.2, 1.8, 0.01, 0.8, 0.1, 0.1) # test case parameters
# initialise variables
bisse.ll = 0;
passe.ll = matrix(0, N.splits,N.splits)
passe.t = matrix(0, N.splits,N.splits)
trees = vector('list', N.reps)

# generate trees with given parameters
for (n in 1:N.reps)
{
  trees[[n]] = fast.bisse(N.species, pars)
  names(trees[[n]]$tip.state) = paste('t', 1L:N.species, sep='')
}

# use microbenchmark to time BiSSE runs on the 100 trees
m = microbenchmark({
  for (n in 1:N.reps)
  {
    lik.s <- make.bisse(trees[[n]], trees[[n]]$tip.state, strict = FALSE)     
    bisse.ll <<- bisse.ll + lik.s(pars)
  }
}, times = 1)
bisse.ll = bisse.ll / N.reps # mean BiSSE LL
bisse.t = m$time/1e9/N.reps # mean BiSSE run time

# for every possible combination of splits, run PASSE
# again timing each run of 100 trees
for (i in 1:N.splits)
{
  for (j in 1:N.splits)
  {
    print(paste(i,j))
    m = microbenchmark({
      for (n in 1:N.reps)
        passe.ll[i,j] = passe.ll[i,j] + 
          passe(pars, 
                trees[[n]], N.species, 
                n1 = splits[i], n2 = splits[j], 
                diag = FALSE, res.val = -Inf)
    }, times = 1)
    passe.t[i,j] = m$time/1e9/N.reps # mean PASSE run time
  }
}
passe.ll = passe.ll / N.reps # mean PASSE LLs

# calculate speedup and LL diff between each PASSE version and BiSSE
speedup = bisse.t / passe.t
ll.diff = bisse.ll - passe.ll

# determine Pareto dominance
ii = rev(order(speedup))
pareto = rep(TRUE, length(ii))
ll.standard = Inf # the standard LL that needs to be beaten
for (i in ii) # go from slowest to fastest PASSE version
{
  if (ll.diff[i] < ll.standard) # if it is more accurate than the standard, set a new standard
    ll.standard = ll.diff[i]
  else # otherwise it's both slower and less accurate, therefore Pareto-dominated
    pareto[i] = FALSE
}
pareto = matrix(pareto, N.splits,N.splits) # unflatten Pareto variable

# generate paired-split names
version.names = paste(splits[row(passe.t[1,,])], splits[col(passe.t[1,,])], sep=',')

# plot Figure 1 from paper
pdf("figure 1.pdf", width = 7, height = 7)
plot(speedup[1,,], ll.diff[1,,], 
     pch = 19, xlab = 'Speedup', ylab = expression(Delta*italic(l)),
     log = 'xy', xlim = c(1,60), ylim = c(3,500), 
     cex = ifelse(pareto[1,,], 1.5, 0.7), col = ifelse(pareto[1,,], 'black', '#C0C0C0'))
text(speedup[1,,], ll.diff[1,,], version.names, pos = ifelse(pareto[1,,], 4, 2),
     cex = ifelse(pareto[1,,], 0.7, 0.5), col = ifelse(pareto[1,,], 'black', '#C0C0C0'))
dev.off()

# output the number of Pareto-dominant with leaf splits ==, < and > the other splits
pareto1 = pareto[1,,]
print(table(pareto1[row(pareto1)==col(pareto1)]))
print(table(pareto1[row(pareto1)>col(pareto1)]))
print(table(pareto1[row(pareto1)<col(pareto1)]))

save.image('pareto.Rdata')

```

We then generated further random trees, this time with 10,000 tip species, each with randomly generated parameters. We set $\lambda_1=1$ as our time scale, and without loss of generality assumed that state 1 has the highest speciation rate, i.e. $\lambda_0<\lambda_1$. The other parameters were taken from the distributions $\lambda_0\sim \mathrm{Unif}(0.1,\lambda_1), \mu_0\sim\exp(\mathrm{Unif}(\log(0.01),\log(0.8\lambda_0))), \mu_1\sim\exp(\mathrm{Unif}(\log(0.01),\log(0.8))), q_{01}\sim \mathrm{Unif}(0.01,0.1)$ and $q_{10}\sim \mathrm{Unif}(0.01,0.1)$. These parameters were chosen to cover a wide range of realistically possible trees without requiring too much computation time, which increases rapidly as $\mu$ approaches $\lambda$ in particular. 

```{r gentrees}
N.reps = 100L
N.species = 10000L
lambda0 = runif(N.reps, 0.1, 1)
mu0 = exp(runif(N.reps, log(0.01), log(0.8 * lambda0)))
mu1 = exp(runif(N.reps, log(0.01), log(0.8)))
q01 = runif(N.reps, 0.01, 0.1)
q10 = runif(N.reps, 0.01, 0.1)

pars = cbind(lambda0, 1, mu0, mu1, q01, q10)
trees = vector('list', N.reps)

for (n in 1:N.reps)
{
  trees[[n]] = fast.bisse(N.species, pars[n,])
  names(trees[[n]]$tip.state) = paste('t', 1L:N.species, sep='')
}
save(pars, trees, file = sprintf('random trees %d.Rdata', run))
```

This process was then repeated, but with trees restricted such that $\lambda_0 = \lambda_1 = 1$ and $\mu_0 = \mu_1$.

```{r genbmtrees}
mu1 = runif(N.reps, 0.01, 0.8)
q01 = runif(N.reps, 0.01, 0.1)
q10 = runif(N.reps, 0.01, 0.1)
pars.bm = cbind(1, 1, mu1, mu1, q01, q10)
trees.bm = vector('list', N.reps)

for (n in 1:N.reps)
{
  trees.bm[[n]] = fast.bisse(N.species, pars.bm[n,])
  names(trees.bm[[n]]$tip.state) = paste('t', 1L:N.species, sep='')
}

save(pars.bm, trees.bm, file = sprintf('random bm trees %d.Rdata', run))
```

We then calculated the log-likelihood of each method for each of the random (full) trees for comparison in Figure 2.

```{r figure2analysis}
bisse.ll = passe.naive.ll = passe.2010.ll = numeric(N.reps)

for (n in 1:N.reps)
{
  tree = trees[[n]]
  lik.s = make.bisse(trees[[n]], trees[[n]]$tip.state, strict = FALSE)     
  bisse.ll[n] = lik.s(pars[n,])
  passe.naive.ll[n] = passe(pars[n,], 
                            trees[[n]], N.species, 
                            n1 = 1L, n2 = 1L, 
                            diag = FALSE, res.val = -Inf)
  passe.2010.ll[n] = passe(pars[n,], 
                           trees[[n]], N.species, 
                           n1 = 20L, n2 = 10L, 
                           diag = FALSE, res.val = -Inf)
}
save(bisse.ll, passe.naive.ll, passe.2010.ll, file = sprintf('LLs %d.Rdata', run))
```

We calculated the maximum-likelihood parameters for each of these trees, or a subset, for BiSSE, naïve PASSÉ and a suitable Pareto-optimal choice of $x,y$-PASSÉ, then compared them with the known parameters to assess their performance across parameter space.

```{r calcmles}
#proc = 0 # 0 1 2 3
procs = 1; proc = 0;
v = 1:N.reps
v = v[v %% procs == proc]
bisse.mle = passe.naive.mle = passe.2010.mle = array(NA, c(2,2,N.reps, 6))
bisse.mle.ll = passe.naive.mle.ll = passe.2010.mle.ll = array(NA, c(2,2,N.reps))
tree.type = c('full', 'rest')
pars.set = list(c(1,3,5,6), 1:6)
passeR = function(x, ...) passe(pars = c(x[1], x[1], x[2], x[2], x[3], x[4]), ...)
lik.s2R = function(x, ...) lik.s2(x = c(x[1], x[1], x[2], x[2], x[3], x[4]), ...)

passe.choose = function(x, type=1, ...) ifelse(type==2, passe(x, ...), passeR(x,...))
lik.s2.choose = function(x, type=1, ...) ifelse(type==2, lik.s2(x, ...), lik.s2R(x,...))

# load trees and trees.bm

for (j in 1:2) # actual space (bm or full trees)
{
  if (j == 1)
  {
    pars.curr = pars.bm
    trees.curr = trees.bm
  }
  else
  {
    pars.curr = pars
    trees.curr = trees
  }
  for (i in 1:2) # search space (bm or full trees)
  {
    t.el = proc.time()[3]
    for (n in v)
    {
      print(n)
      res = -Inf
      res.new = -1e9
      while (res.new > res + 1e-4)
      {
        if (i == 1) 
          pars.start = pars.curr[n,pars.set[[i]],drop=FALSE] # start from actual bm tree value
        else
          pars.start = passe.naive.mle[1,j,n,] # start from restricted optimum (should ensure likelihood ratio < 1)
        O = optim(pars.start, passe.choose, tree = trees.curr[[n]], n_species = N.species, n1 = 1L, n2 = 1L, control = list(fnscale = -1), type = i)
        res = res.new
        res.new = O$value
      }
      if (i == 1)
        passe.naive.mle[i,j,n,] = O$par[c(1,1,2,2,3,4)]
      else
        passe.naive.mle[i,j,n,] = O$par
      passe.naive.mle.ll[i,j,n] = O$value
    }
    t.el = proc.time()[3] - t.el
    print(t.el)
    
    t.el = proc.time()[3]
    for (n in v)
    {
      print(n)
      res = -Inf
      res.new = -1e9
      while (res.new > res + 1e-4)
      {
        if (i == 1) 
          pars.start = pars.curr[n,pars.set[[i]],drop=FALSE]
        else
          pars.start = passe.2010.mle[1,j,n,] # start from restricted optimum (should ensure likelihood ratio < 1)
        O = optim(pars.start, passe.choose, tree = trees.curr[[n]], n_species = N.species, n1 = 20L, n2 = 10L, control = list(fnscale = -1), type = i)
        res = res.new
        res.new = O$value
      }
      if (i == 1)
        passe.2010.mle[i,j,n,] = O$par[c(1,1,2,2,3,4)]
      else
        passe.2010.mle[i,j,n,] = O$par
      passe.2010.mle.ll[i,j,n] = O$value
    }
    t.el = proc.time()[3] - t.el
    print(t.el)
    
    t.el = proc.time()[3]
    for (n in v)
    {
      lik.s = make.bisse(trees.curr[[n]], trees.curr[[n]]$tip.state, strict = FALSE)   
      lik.s2 = function(x) lik.s(abs(x))
      res = -Inf
      res.new = -1e9
      while (res.new > res + 1e-4)
      {
        if (i == 1) 
          pars.start = pars.curr[n,pars.set[[i]],drop=FALSE]
        else
          pars.start = bisse.mle[1,j,n,]
        O = optim(pars.curr[n,pars.set[[i]],drop=FALSE], lik.s2.choose, control = list(fnscale = -1, maxit = 1000), type = i)
        res = res.new
        res.new = O$value
      }
      if (i == 1)
        bisse.mle[i,j,n,] = O$par[c(1,1,2,2,3,4)]
      else
        bisse.mle[i,j,n,] = O$par
      bisse.mle.ll[i,j,n] = O$value
    }
    t.el = proc.time()[3] - t.el
    print(t.el)
  }
}
save(bisse.mle, passe.naive.mle, passe.2010.mle,
     bisse.mle.ll, passe.naive.mle.ll, passe.2010.mle.ll,  
     file = sprintf('mles %d.Rdata', run))
```

We then stitched together results of multiple runs of previously performed 1000 tree analyses to save memory - storing trees is memory-intensive.

```{r stitching}
N.reps = 20L
v = 1:N.reps
N.species = 10000L
N.runs = 500L
L = list.files('bee069', pattern = 'mle.*')
which.runs = sort(as.integer(gsub('[^0-9]', '', L)))
bisse.mle.all = passe.naive.mle.all = passe.2010.mle.all = array(NA, c(2,2,N.reps*N.runs, 6))
bisse.mle.ll.all = passe.naive.mle.ll.all = passe.2010.mle.ll.all = array(NA, c(2,2,N.reps*N.runs))

bisse.ll.all = passe.naive.ll.all = passe.2010.ll.all = matrix(NA, N.reps*N.runs)
pars.all = matrix(NA, N.reps*N.runs, 6)
pars.bm.all = matrix(NA, N.reps*N.runs, 6)
tree.length = numeric(nrow(pars.all))

for (i in 1:N.runs)
{
  print(i)
  runs = which.runs[i]
  load(sprintf('bee069/mles %d.Rdata', runs))
  
  passe.naive.mle.ll.all[,,v + N.reps*(i-1)] = passe.naive.mle.ll
  passe.2010.mle.ll.all[,,v + N.reps*(i-1)] = passe.2010.mle.ll
  bisse.mle.ll.all[,,v + N.reps*(i-1)] = bisse.mle.ll
  
  passe.naive.mle.all[,,v + N.reps*(i-1),] = passe.naive.mle
  passe.2010.mle.all[,,v + N.reps*(i-1),] = passe.2010.mle
  bisse.mle.all[,,v + N.reps*(i-1),] = bisse.mle
  
  load(sprintf('bee069/LLs %d.Rdata', runs))
  
  passe.naive.ll.all[v + N.reps*(i-1)] = passe.naive.ll
  passe.2010.ll.all[v + N.reps*(i-1)] = passe.2010.ll
  bisse.ll.all[v + N.reps*(i-1)] = bisse.ll
  
  load(sprintf('bee069/random trees %d.Rdata', runs))
  pars.all[v + N.reps*(i-1),] = pars
  for (j in 1:N.reps)
    tree.length[j + N.reps * (i-1)] = max(node.depth.edgelength(trees[[j]]))
  
  
  load(sprintf('bee069/random bm trees %d.Rdata', runs))
  pars.bm.all[v + N.reps*(i-1),] = pars.bm
}

passe.naive.mle.all = abs(passe.naive.mle.all)
passe.2010.mle.all = abs(passe.2010.mle.all)
bisse.mle.all = abs(bisse.mle.all)

```

## Figure 2

Figure 2 shows the accuracy of different choices of parameter value for the 10,000 tip trees using naïve PASSÉ. Smaller values of $\mu_1$ are clearly more accurate – this is likely to be the main effect because it is associated with the state with the highest speciation rate ($\lambda_1=1$). Other effects are less obvious – there is also some effect of increasing accuracy with decreasing $\mu_0$. 


```{r figure 2}
pdf('figure 2 naive passe.pdf', 12, 12)
par(cex = 1.5, lwd = 2, cex.axis = 2, las = 2)
palette(viridis(1024))
cols = log10(bisse.ll.all - passe.naive.ll.all)
cols = (cols - min(cols)) / (max(cols)-min(cols))*1023 + 1
pairs(pars.all[,-2], 
      labels = c(expression(lambda[0]), expression(mu[0]), expression(mu[1]), 
                 expression(q["01"]), expression(q[10])), label.pos = 0.5, 
      lower.panel = NULL, pch=21, bg = cols, col = 'black', cex = pairs.cex, cex.labels = 10, lwd = pairs.lwd)
ticks = c(100,200,500,1000,2000,5000); 
image.plot(legend.only = TRUE, smallplot = c(0.1, 0.17, 0.1, 0.5), 
           col = palette(viridis(1024)), 
           zlim = range(log10(bisse.ll.all - passe.naive.ll.all)), 
           legend.lab = "Difference in\nlog-likelihood", 
           legend.cex = 2, legend.line = 6, #legend.args = list(side = 4, las = 4),
           axis.args = list(at = log10(ticks), labels = ticks))
dev.off()
```

```{r cube}
lambda.max = 0.23
mu.max = 0.83
q.max = 0.19
v1 = seq(0, lambda.max, length.out = 101)
v2 = seq(0, mu.max, length.out = 101)
v3 = seq(0, q.max, length.out = 101)

face1 = cbind(as.vector(v1 %o% (0*v2 + 1)), as.vector((0*v1 + 1) %o% v2), 0)
face2 = cbind(as.vector(v1 %o% (0*v2 + 1)), as.vector((0*v1 + 1) %o% v2), q.max)
face3 = cbind(as.vector(v1 %o% (0*v3 + 1)), 0, as.vector((0*v1 + 1) %o% v3))
face4 = cbind(as.vector(v1 %o% (0*v3 + 1)), mu.max, as.vector((0*v1 + 1) %o% v3))
face5 = cbind(0, as.vector(v2 %o% (0*v3 + 1)), as.vector((0*v2 + 1) %o% v3))
face6 = cbind(lambda.max, as.vector(v2 %o% (0*v3 + 1)), as.vector((0*v2 + 1) %o% v3))

faces = rbind(face1, face2, face3, face4, face5, face6)

cube.cols = rgb((faces[,1]/lambda.max)^gamma, (faces[,2]/mu.max)^gamma, (faces[,3]/q.max)^gamma)
plot3d(faces, col = cube.cols)
```

## Figure 3

Figure 3 shows how accuracy of parameter estimation changes between BiSSE, naïve PASSÉ and 20,10-PASSÉ. The main error in BiSSE (Figure 3a) is in $q_{01}$ and $q_{10}$, and appears to be worst where $\mu_0, \mu_1,$ or $\lambda_0$ are small or when $q_{01}$ is large. This is likely because these parameters would cause high levels of imbalance between numbers of species in the two states, making estimation of transition rates more difficult. Errors are much higher for naïve PASSÉ (Figure 3b) with particularly high errors in speciation and extinction rates where $\mu_0$ or $\mu_1$ are high, along with the issues with transition rates seen in BiSSE. Increasing the number of timesteps as in $20,10$-PASSÉ (Figure 3c) makes the speciation and extinction rate errors lower, though still much higher than for BiSSE.

```{r figure3a}

gamma = 0.5

passe.naive.mle.all.scale = passe.naive.mle.all[2,2,,]/passe.naive.mle.all[2,2,,2]
print(apply(abs(pars.all - passe.naive.mle.all.scale), 2, range))
print(apply(abs(pars.all - passe.naive.mle.all.scale), 2, quantile, probs = 0.95))
print(apply(abs(pars.all - passe.naive.mle.all.scale), 2, mean))

# cols = rgb(pmax(abs(bisse.mle.all[,1] - pars.all[,1]), abs(bisse.mle.all[,2] - pars.all[,2]))/0.22,
#            pmax(abs(bisse.mle.all[,3] - pars.all[,3]), abs(bisse.mle.all[,4] - pars.all[,4]))/0.41,
#            pmax(abs(bisse.mle.all[,5] - pars.all[,5]), abs(bisse.mle.all[,6] - pars.all[,6]))/0.22)

cols = rgb(pmin(pmax(abs(passe.naive.mle.all.scale[,1] - pars.all[,1]), abs(passe.naive.mle.all.scale[,2] - pars.all[,2]))/lambda.max,1)^gamma,
           pmin(pmax(abs(passe.naive.mle.all.scale[,3] - pars.all[,3]), abs(passe.naive.mle.all.scale[,4] - pars.all[,4]))/mu.max,1)^gamma,
           pmin(pmax(abs(passe.naive.mle.all.scale[,5] - pars.all[,5]), abs(passe.naive.mle.all.scale[,6] - pars.all[,6]))/q.max,1)^gamma)

pdf('figure 3 naive passe.pdf', 12, 12)
par(cex = 1.5, lwd = 2, cex.axis = 2, las = 2)
pairs(pars.all[,-2], 
      labels = c(expression(lambda[0]), expression(mu[0]), expression(mu[1]), 
                 expression(q["01"]), expression(q[10])), label.pos = 0.5, 
      lower.panel = NULL, pch=21, bg = cols, col = 'black', cex = pairs.cex, cex.labels = 10, lwd = pairs.lwd,
      diag.panel = function(x,...) {points(x, passe.naive.mle.all.scale[,-2][,which(x[1] == pars.all[1,-2])], 
                                           col = '#80808040', pch = 19); abline(c(0,1), col='red')})
dev.off()
```

```{r figure3b}
passe.2010.mle.all.scale = passe.2010.mle.all[2,2,,]/passe.2010.mle.all[2,2,,2]
print(apply(abs(pars.all - passe.2010.mle.all.scale), 2, range))
print(apply(abs(pars.all - passe.2010.mle.all.scale), 2, quantile, probs = 0.95))
print(apply(abs(pars.all - passe.2010.mle.all.scale), 2, mean))

# cols = rgb(pmax(abs(bisse.mle.all[,1] - pars.all[,1]), abs(bisse.mle.all[,2] - pars.all[,2]))/0.22,
#            pmax(abs(bisse.mle.all[,3] - pars.all[,3]), abs(bisse.mle.all[,4] - pars.all[,4]))/0.41,
#            pmax(abs(bisse.mle.all[,5] - pars.all[,5]), abs(bisse.mle.all[,6] - pars.all[,6]))/0.22)

cols = rgb(pmin(pmax(abs(passe.2010.mle.all.scale[,1] - pars.all[,1]), abs(passe.2010.mle.all.scale[,2] - pars.all[,2]))/lambda.max,1)^gamma,
           pmin(pmax(abs(passe.2010.mle.all.scale[,3] - pars.all[,3]), abs(passe.2010.mle.all.scale[,4] - pars.all[,4]))/mu.max,1)^gamma,
           pmin(pmax(abs(passe.2010.mle.all.scale[,5] - pars.all[,5]), abs(passe.2010.mle.all.scale[,6] - pars.all[,6]))/q.max,1)^gamma)

pdf('figure 3 20,10-passe.pdf', 12, 12)
par(cex = 1.5, lwd = 2, cex.axis = 2, las = 2)
pairs(pars.all[,-2], 
      labels = c(expression(lambda[0]), expression(mu[0]), expression(mu[1]), 
                 expression(q["01"]), expression(q[10])), label.pos = 0.5, 
      lower.panel = NULL, pch=21, bg = cols, col = 'black', cex = pairs.cex, cex.labels = 10, lwd = pairs.lwd,
      diag.panel = function(x,...) {points(x, passe.2010.mle.all.scale[,-2][,which(x[1] == pars.all[1,-2])], 
                                           col = '#80808040', pch = 19); abline(c(0,1), col='red')})

dev.off()
```

```{r figure3c}
bisse.mle.all.scale = bisse.mle.all[2,2,,]/bisse.mle.all[2,2,,2]
print(apply(abs(pars.all - bisse.mle.all.scale), 2, range))
print(apply(abs(pars.all - bisse.mle.all.scale), 2, quantile, probs = 0.95))
print(apply(abs(pars.all - bisse.mle.all.scale), 2, mean))

# cols = rgb(pmax(abs(bisse.mle.all[,1] - pars.all[,1]), abs(bisse.mle.all[,2] - pars.all[,2]))/0.22,
#            pmax(abs(bisse.mle.all[,3] - pars.all[,3]), abs(bisse.mle.all[,4] - pars.all[,4]))/0.41,
#            pmax(abs(bisse.mle.all[,5] - pars.all[,5]), abs(bisse.mle.all[,6] - pars.all[,6]))/0.22)

cols = rgb(pmin(pmax(abs(bisse.mle.all.scale[,1] - pars.all[,1]), abs(bisse.mle.all.scale[,2] - pars.all[,2]))/lambda.max,1)^gamma,
           pmin(pmax(abs(bisse.mle.all.scale[,3] - pars.all[,3]), abs(bisse.mle.all.scale[,4] - pars.all[,4]))/mu.max,1)^gamma,
           pmin(pmax(abs(bisse.mle.all.scale[,5] - pars.all[,5]), abs(bisse.mle.all.scale[,6] - pars.all[,6]))/q.max,1)^gamma)

pdf('figure 3 bisse.pdf', 12, 12)
par(cex = 1.5, lwd = 2, cex.axis = 2, las = 2)
pairs(pars.all[,-2], 
      labels = c(expression(lambda[0]), expression(mu[0]), expression(mu[1]), 
                 expression(q["01"]), expression(q[10])), label.pos = 0.5, 
      lower.panel = NULL, pch=21, bg = cols, col = 'black', cex = pairs.cex, cex.labels = 10, 
      lwd = pairs.lwd,
      diag.panel = function(x,...) {points(x, bisse.mle.all.scale[,-2][,which(x[1] == pars.all[1,-2])], 
                                           col = '#80808040', pch = 19); abline(c(0,1), col='red')})
dev.off()
```

```{r nn}
library(reticulate)
use_condaenv("keras-gpu")
library(keras)

cv.folds = 10
cv.fold = sample(rep(1:cv.folds, ceiling(nrow(pars.all)/cv.folds))[1:nrow(pars.all)])

err = 0
test.pred.full = matrix(NA, nrow(pars.all), 5)

hidden = 15

hiddens = c(15, 20, 25, 30)

val.out = matrix(NA, 5, 100)

# switch half of lambda_0 and lambda_1
# switches = sample.int(10000, 5000)
# 
# pars.all.switch = pars.all
# passe.naive.mle.all.switch = passe.naive.mle.all
# pars.all.switch[switches,1] = pars.all[switches,2]
# pars.all.switch[switches,2] = pars.all[switches,1]
# pars.all.switch = pars.all.switch / pars.all.switch[,2] # re-normalise new lambda_1 to 1
# 
# passe.naive.mle.all.switch[2,2,switches,1] = passe.naive.mle.all[2,2,switches,2]
# passe.naive.mle.all.switch[2,2,switches,2] = passe.naive.mle.all[2,2,switches,1]
# 

setup.fold = function(fold, i, j, predict.pars = FALSE)
{
  l0 = passe.naive.mle.all[i,j,,1]
  l1 = passe.naive.mle.all[i,j,,2]
  switch = l0 > l1
  
  print(paste('fold:',fold))
  vv <<- which(cv.fold != fold) 
  
  # training inputs
  data.train = passe.naive.mle.all[i,j,vv,]
  switch.train = which(switch[vv]) # where lambda_0 is (wrongly) predicted higher than lambda_1 by naive passe
  data.train[switch.train,] = data.train[switch.train, c(2,1,4,3,6,5)] # switch states for such cases so that input is where highest predicted lambda is state 1
  data.train = data.train / data.train[,2] # normalise by lambda_1
  data.train = data.train[,-2]
  data.train.sd <<- apply(data.train, 2, sd)
  data.train.mean <<- apply(data.train, 2, mean)
  data.train.norm = t((t(data.train) - data.train.mean)/data.train.sd)
  
  # log-likelihoods
  ll.train = passe.naive.mle.ll.all[i,j,vv]
  ll.train = ll.train - (N.species-2) * log(l1[vv]) # normalise by lambda_1 
  ll.train.mean <<- mean(ll.train)
  ll.train.sd <<- sd(ll.train)
  ll.train.norm = (ll.train - ll.train.mean) / ll.train.sd
  
  # combined training inputs
  df.train <<- cbind(data.train.norm, ll.train.norm)
  
  # training outputs
  if (predict.pars)
    pars.train = pars.all[vv,]
  else
    pars.train = bisse.mle.all[i,j,vv,] #pars.curr[vv,]
  
  pars.train[switch.train,] =  pars.train[switch.train, c(2,1,4,3,6,5)] # match states with training inputs
  pars.train = pars.train / pars.train[,2] # normalise by predicted highest by naive passe
  pars.train = pars.train[,-2]
  pars.train.sd <<- apply(pars.train, 2, sd)
  pars.train.mean <<- apply(pars.train, 2, mean)
  pars.train.norm <<- t((t(pars.train) - pars.train.mean)/pars.train.sd)
  
  # testing outputs
  vv.test <<- which(cv.fold == fold)
  if (predict.pars)
    pars.test = pars.all[vv.test,]
  else
    pars.test = bisse.mle.all[i,j,vv.test,] # pars.curr[vv.test,]
  
  if (j == 2)
  {
    switch.test = which(switch[vv.test])
    pars.test[switch.test,] =  pars.test[switch.test, c(2,1,4,3,6,5)] # match states with training inputs
  }
  pars.test = pars.test / pars.test[,2] # normalise by predicted highest by naive passe
  pars.test = pars.test[,-2]
  pars.test.norm <<- t((t(pars.test) - pars.train.mean)/pars.train.sd)
  
  # testing inputs
  data.test = passe.naive.mle.all[i,j,vv.test,]
  if (j==2) data.test[switch.test,] = data.test[switch.test, c(2,1,4,3,6,5)] # match states with inputs
  data.test = data.test / data.test[,2]  # normalise by predicted highest by naive passe
  data.test = data.test[,-2]
  data.test.norm = t((t(data.test) - data.train.mean)/data.train.sd)
  
  #log-likelihoods (don't need to deal with switching)
  ll.test = passe.naive.mle.ll.all[i,j,vv.test]
  ll.test = ll.test - (N.species-2) * log(l1[vv.test]) # normalise by lambda_1 
  ll.test.norm = (ll.test - ll.train.mean) / ll.train.sd
  
  # combined test inputs
  df.test <<- cbind(data.test.norm, ll.test.norm)
}

# Test for single layers 15 - 30
# setup.fold(1)
# for (j in 1:4)
# {
#   hidden = hiddens[j]
#   print(paste('test for', hidden, 'hidden neurons'))
#   model = keras_model_sequential()
#   layer_dense(model, units = hidden, input_shape = c(6), activation = 'sigmoid')
#   layer_dense(model, units = 5)
#   compile(model, optimizer = 'adam', loss = 'mean_squared_error', metrics = c('mean_squared_error'))
#   history = fit(model, df.train, pars.train.norm,
#                 epochs = 100, batch_size = 32,
#                 validation_data = list(df.test, pars.test.norm), verbose = 0, view_metrics = TRUE)
#   val.out[j,] = history$metrics$val_mean_squared_error
# }
# 
# model = keras_model_sequential()
# layer_dense(model, units = 25, input_shape = c(6), activation = 'sigmoid')
# layer_dense(model, units = 10, activation = 'sigmoid')
# layer_dense(model, units = 5)
# compile(model, optimizer = 'adam', loss = 'mean_squared_error', metrics = c('mean_squared_error'))
# history = fit(model, df.train, pars.train.norm,
#               epochs = 100, batch_size = 32,
#               validation_data = list(df.test, pars.test.norm), verbose = 0, view_metrics = TRUE)
# val.out[5,] = history$metrics$val_mean_squared_error
# matplot(t(val.out))
# matplot(t(val.out[,80:100]))
# 
# print(val.out[,100])

# now do it for single hidden layer of 25 with all 10 folds
passe.nn.mle.all = array(NA, c(2,2,10000,6))
passe.nn.mle.ll.all = array(NA, c(2,2,10000))
pars.set = list(c(2,4,5,6), 1:6)
pars.set2 = list(c(2,4,5), 1:5)

# for corrected naive PASSE MLEs, set i=j=2 and predict.pars = TRUE

for (i in 1:2) # search space 1 = bm, 2 = all
{
  for (j in 1:2) # actual tree 1 = bm, 2 = all
  {
    print(paste(i,',',j))
    test = matrix(NA, 10000, ifelse(i==2, 5, 3))
    for (fold in 1:10)
    {
      setup.fold(fold, i, j)
      model = keras_model_sequential()
      layer_dense(model, units = 25, input_shape = ifelse(i == 2, 6, 4), activation = 'sigmoid')
      layer_dense(model, units = ifelse(i==2, 5, 3))
      compile(model, optimizer = 'adam', loss = 'mean_squared_error', metrics = c('mean_squared_error'))
      history = fit(model, df.train[,pars.set[[i]]], pars.train.norm[,pars.set2[[i]]],
                    epochs = ifelse(i==2, 100, 20), batch_size = 32,
                    validation_data = list(df.test[,pars.set[[i]]], pars.test.norm[,pars.set2[[i]]]), verbose = 0, view_metrics = TRUE)
      test[vv.test, ] = predict(model, df.test[,pars.set[[i]]])
      test[vv.test, ] = t(t(test[vv.test, ]) * pars.train.sd[pars.set2[[i]]] + pars.train.mean[pars.set2[[i]]])
      
      if (i == 2)
      {
        switch.test = vv.test[switch[vv.test]]
        if (length(switch.test) > 0)
        {
          test[switch.test,] = test[switch.test, c(1,3,2,5,4)] # reswitch so states are in right places 
          # (i.e. lambda1 is actual max lambda, not max predicted by naive passe)
          test[switch.test, -1] = test[switch.test, -1]/test[switch.test, 1] # scale by (actual) lambda1
          test[switch.test, 1] = 1/test[switch.test, 1] # convert lambda1 to lambda0
        }
        
      }
      if (i == 2)
      {
        passe.nn.mle.all[i,j,vv.test,-2] = test[vv.test,]
        passe.nn.mle.all[i,j,vv.test,2] = 1
      }
      else
      {
        passe.nn.mle.all[i,j,vv.test,-(1:2)] = test[vv.test,c(1,1,2,3)]
        passe.nn.mle.all[i,j,vv.test,1:2] = 1
      }
    }
  }
}

save(passe.nn.mle.all, file='nn mles.Rdata')
#test.unswitch = test
# test.unswitch[switches,1] = test[switches,2]
# test.unswitch[switches,2] = test[switches,1]


```

```{r figure3d}
test.pred.full2 = cbind(test[,1], 1, test[,2:5])
print(apply(abs(pars.all - test.pred.full2), 2, range))
print(apply(abs(pars.all - test.pred.full2), 2, quantile, probs = 0.95))
print(apply(abs(pars.all - test.pred.full2), 2, mean))

cols = rgb(pmin(pmax(abs(test.pred.full2[,1] - pars.all[,1]), abs(test.pred.full2[,2] - pars.all[,2]))/lambda.max,1)^gamma,
           pmin(pmax(abs(test.pred.full2[,3] - pars.all[,3]), abs(test.pred.full2[,4] - pars.all[,4]))/mu.max,1)^gamma,
           pmin(pmax(abs(test.pred.full2[,5] - pars.all[,5]), abs(test.pred.full2[,6] - pars.all[,6]))/q.max,1)^gamma)

pdf('figure 3 naive passe nn.pdf', 12, 12)
par(cex = 1.5, lwd = 2, cex.axis = 2, las = 2)
pairs(pars.all[,-2], 
      labels = c(expression(lambda[0]), expression(mu[0]), expression(mu[1]), 
                 expression(q["01"]), expression(q[10])), label.pos = 0.5, 
      lower.panel = NULL, pch=21, bg = cols, col = 'black', cex = pairs.cex, cex.labels = 10, lwd = pairs.lwd, 
      diag.panel = function(x,...) {points(x, test[,which(x[1] == pars.all[1,-2])], 
                                           col = '#80808040', pch = 19); abline(c(0,1), col='red')})

dev.off()
```

```{r compare}
apply(abs(bisse.mle.all.scale - pars.all) > abs(passe.naive.mle.all.scale - pars.all), 2, sum)/10000
apply(abs(bisse.mle.all.scale - pars.all) > abs(passe.2010.mle.all.scale - pars.all), 2, sum)/10000
apply(abs(bisse.mle.all.scale - pars.all) > abs(test.pred.full2 - pars.all), 2, sum)/10000
```

```{r figure4a}
pvals = pchisq(2*(passe.naive.mle.ll.all[2,2,] - passe.naive.mle.ll.all[1,2,]), 2, lower.tail = FALSE)

plot.fig4 = function(filename){
  pdf(filename, 12, 12)
  par(cex = 1.5, lwd = 2, cex.axis = 2, las = 2)
  pal = viridis(1024)[1:512]
  pal2 = c(viridis(1024)[1:512], 'yellow', 'red', 'black')
  cols = log10(pvals)
  cols[cols < -100] = -100
  cols.scale = (cols - min(cols, na.rm = TRUE)) / (log10(0.01)-min(cols, na.rm = TRUE))*511 + 1
  cols.scale[pvals > 0.01] = 513
  cols.scale[pvals > 0.05] = 514
  cols.scale[pvals == 1] = 515
  pvals[pvals == 1] = NA
  col.idx = 1:length(pvals)#(order(pvals))
  palette(pal2)
  pairs(pars.all[col.idx,-2], 
        labels = c(expression(lambda[0]), expression(mu[0]), expression(mu[1]), 
                   expression(q["01"]), expression(q[10])), label.pos = 0.5, 
        lower.panel = NULL, pch=21, 
        bg = cols.scale[col.idx], 
        col = 'black', cex = pairs.cex, cex.labels = 10, lwd = pairs.lwd,
        xaxt = 'n', yaxt = 'n',
        upper.panel = function(x, y, ...) {points(x,y,...); 
          if (x[1] == pars.all[1,6]) axis(4); 
          if (y[1] == pars.all[1,1]) axis(3); },
        diag.panel = function(x,...) {
          props = aggregate(pvals > 0.05, by = list(quantile = (rank(x)-1)%/%100 + 1), FUN = mean, na.rm = TRUE)$x
          widths = diff(quantile(x, probs = seq(0,1,0.01)))
          barplot(props * diff(range(x)), widths,
                  space = c(min(x)/mean(widths), rep(0,99)), 
                  xlim = range(x), pos = 0, offset = min(x),
                  add = TRUE, axes = FALSE, border = NA)
          axis(1)
          axis(2, col = 'red', labels = seq(0,.75,0.25), 
               at = seq(0,.75,0.25)*diff(range(x)) + min(x))
        }
  )
  ticks = c(1e-100, 1e-50, 1e-20, 1e-10, 1e-5, 0.01, 0.05, 0.1); 
  vec = 10^(-(10^seq(2, 0, length.out = 512)))
  cols2 = log10(vec)
  cols2.scale = (cols2 - min(cols2, na.rm = TRUE)) / (log10(0.01)-min(cols2, na.rm = TRUE))*511 + 1
  cols2.scale[vec > 0.01] = 513
  cols2.scale[vec > 0.05] = 514
  
  image.plot(legend.only = TRUE, smallplot = c(0.1, 0.17, 0.1, 0.5), 
             col = cols2.scale, 
             zlim = -log(-log(range(ticks))), 
             legend.lab = 'p-value', 
             legend.cex = 3, legend.line = 7,
             axis.args = list(at = -log(-log(ticks)), labels = ticks))
  dev.off()
}

plot.fig4('figure 4 naive passe.pdf')
```

```{r figure4b}
pvals = pchisq(2*(bisse.mle.ll.all[2,2,] - bisse.mle.ll.all[1,2,]), 2, lower.tail = FALSE)
plot.fig4('figure 4 bisse.pdf')
```
```{r figure4c}
passe.nn.mle.ll.all = array(NA, c(2,2,10000))
library(dplyr)
p = progress_estimated(N.reps*N.runs*4)
for (j in 1:2)
{
  for (m in 1:N.runs)
  {
    runs = which.runs[m]
    if (j == 1)
    {
      load(sprintf('bee069/random bm trees %d.Rdata', runs))
      trees.curr = trees.bm
    }
    else
    {
      load(sprintf('bee069/random trees %d.Rdata', runs))
      trees.curr = trees
    } 
    
    for (i in 1:2)
    {
      #print(paste(i,',',j))
      
      for (n in 1:N.reps)
      {
        lik.s = make.bisse(trees.curr[[n]], trees.curr[[n]]$tip.state, strict = FALSE)   
        lik.s2 = function(x) lik.s(abs(x))
        passe.nn.mle.ll.all[i,j,n + N.reps*(m-1)] = lik.s2(passe.nn.mle.all[i,j,n + N.reps*(m-1),])
        p$tick()$print()
      }
    }
  }
}
save(passe.nn.mle.ll.all, file='nn lls.Rdata')

pvals = pchisq(2*(passe.nn.mle.ll.all[2,2,] - passe.nn.mle.ll.all[1,2,]), 2, lower.tail = FALSE)
plot.fig4('figure 4 passe nn.pdf')

```

```{r figure5}
pdf('figure 5.pdf', 6, 6)
bisse.pvals.fp = pchisq(2*(bisse.mle.ll.all[2,1,] - bisse.mle.ll.all[1,1,]), 2, lower.tail = FALSE)
passe.naive.pvals.fp = pchisq(2*(passe.naive.mle.ll.all[2,1,] - passe.naive.mle.ll.all[1,1,]), 2, lower.tail = FALSE)
passe.2010.pvals.fp = pchisq(2*(passe.2010.mle.ll.all[2,1,] - passe.2010.mle.ll.all[1,1,]), 2, lower.tail = FALSE)
nq = 20

x = pars.bm.all[,3]
props = aggregate(passe.naive.pvals.fp < 0.05, by = list(quantile = (rank(x)-1)%/%(10000/nq) + 1), FUN = mean, na.rm = TRUE)$x
widths = diff(quantile(x, probs = seq(0,1,1/nq)))

plot(min(x)+c(0,cumsum(widths)), props[c(1:nq, nq)], type = 's',
     axes = FALSE, col = '#FF8080', ylim = c(0,0.6), xlim = c(0,0.8), lwd = 3,
     xlab = expression(mu), ylab = 'False positive rate')

# barplot(props * diff(range(x)), widths,
#         space = c(min(x)/mean(widths), rep(0,nq-1)), 
#         xlim = range(x), offset = min(x), 
#         axes = FALSE, col = '#FF8080', ylim = c(0,0.5))


props = aggregate(passe.naive.pvals.fp < 0.01, by = list(quantile = (rank(x)-1)%/%(10000/nq) + 1), FUN = mean, na.rm = TRUE)$x

lines(min(x)+c(0,cumsum(widths)), props[c(1:nq, nq)], type = 's',
     col = '#D0D040', lwd = 3)

# barplot(props * diff(range(x)), widths,
#         space = c(min(x)/mean(widths), rep(0,nq-1)), 
#         xlim = range(x), offset = min(x),  
#         add = TRUE, col = '#FFFF80', axes = FALSE)

props = aggregate(passe.2010.pvals.fp < 0.05, by = list(quantile = (rank(x)-1)%/%(10000/nq) + 1), FUN = mean, na.rm = TRUE)$x

# barplot(props * diff(range(x)), widths,
#         space = c(min(x)/mean(widths), rep(0,nq-1)), 
#         xlim = range(x), offset = min(x),  
#         add = TRUE, col = '#00800080', axes = FALSE)

lines(min(x)+c(0,cumsum(widths)), props[c(1:nq, nq)], type = 's',
     col = '#008000', lwd = 3)

props = aggregate(passe.2010.pvals.fp < 0.01, by = list(quantile = (rank(x)-1)%/%(10000/nq) + 1), FUN = mean, na.rm = TRUE)$x

lines(min(x)+c(0,cumsum(widths)), props[c(1:nq, nq)], type = 's',
     col = '#000080', lwd = 3)

props = aggregate(bisse.pvals.fp < 0.05, by = list(quantile = (rank(x)-1)%/%(10000/nq) + 1), FUN = mean, na.rm = TRUE)$x

# barplot(props * diff(range(x)), widths,
#         space = c(min(x)/mean(widths), rep(0,nq-1)), 
#         xlim = range(x), offset = min(x), 
#         add = TRUE, col = '#80808080', axes = FALSE)

lines(min(x)+c(0,cumsum(widths)), props[c(1:nq, nq)], type = 's',
     col = '#808080', lwd = 3)

axis(1, at = seq(0, 0.8, 0.2))
axis(2, at = seq(0, 0.6, 0.1))
box()
abline(h = 0.05, lwd = 3, lty = 2)
legend(x = 'topleft', lwd = 3, col = c('#FF8080', '#D0D040', '#008000', '#000080', '#808080'), legend = c('Naïve PASSÉ p=0.05', 'Naïve PASSÉ p=0.01', '20,10-PASSÉ p=0.05', '20,10-PASSÉ p=0.01', 'BiSSE p=0.05'))

dev.off()


aggregate(bisse.pvals.fp<0.05, by = list(floor(pars.bm.all[,3]*5)), FUN = mean)
aggregate(passe.naive.pvals.fp<0.05, by = list(floor(pars.bm.all[,3]*5)), FUN = mean)

```

```{r cleanup}
dyn.unload('passe.dll')
dyn.unload('passepc.dll')
dyn.unload('passepc2.dll')
```